{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafbfc57-d47d-4559-a948-ac940c5c81b5",
   "metadata": {},
   "source": [
    "# Logistic Regression From Scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a2905-f537-46af-b583-dde421cdb684",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc4aa5e-ef71-4501-ac21-67f7cccdbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c10bd93d-6755-4666-92ae-d395ca9af570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    # Initializes the Logistic Regression model with learning rate and number of iterations\n",
    "    def __init__(self, lr=0.1, n_iters=5000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    # Sigmoid function: Converts linear model output to probabilities between 0 and 1\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # Fits the Logistic Regression model to the training data\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Performs gradient descent for the specified number of iterations\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "\n",
    "            # Computes gradients for weights and bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Updates weights and bias using the gradients and learning rate\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    # Predicts class labels (0 or 1) for input data\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1b531-7a7f-4972-afa4-eea6dfb1ee77",
   "metadata": {},
   "source": [
    "# Model Interpretation and Memo for Logistic Regression (AssemblyAI, 2022)\n",
    "#### Initializes the Logistic Regression model with learning rate and number of iterations\n",
    "    def __init__(self, lr=0.01, n_iters=5000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "The __init__ method initializes the logistic regression model. It sets the learning rate (lr), which controls the step size during optimization, and the number of iterations (n_iters) for gradient descent. It also initializes weights and bias to None, which will later store the model parameters. The learning rate and iteration frequencies (n_iters) can be changed based on your research requirement. The lower the learning rate and higher the iteration, the more precise your model will be. However, the training process may slow down. For a small sample dataset like in the present study did not change the outcome at all.\n",
    "\n",
    "#### sigmoid [1 / (1 + e^-value)](AssemblyAI, 2022)\n",
    "\n",
    "    # Sigmoid function: Converts linear model output to probabilities between 0 and 1\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "The sigmoid function is a mathematical function used in logistic regression to ensure the output falls between 0 and 1, representing a probability. It belongs to a group of functions known as logistic functions, which share this property (Brownlee, 2016). The sigmoid method applies the sigmoid activation function to a given input z. It maps any real number into a range between 0 and 1, making it useful for converting linear model outputs into probabilities (AssemblyAI, 2022).\n",
    "\n",
    "#### fitting (AssemblyAI, 2022)\n",
    "    # Fits the Logistic Regression model to the training data\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "Similarly to the Linear Regression model, this is where the weight and bias are initially set to zero for the gradient descent loop that starts at the next code block. This process is repeated until the optimal balance is maintained between the two. Detailed description of how the gradient descent process work can be found in my transcript under the Linear Regression model. Based the coefficient estimates here allows to move onto the prediction.\n",
    "\n",
    "#### predicting (AssemblyAI, 2022)\n",
    "    # Predicts class labels (0 or 1) for input data\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\n",
    "This code defines the prediction set up in the \"LogisticRegression\" class and tells the model to use the X variable to predict. np.dot() function provides the dot function, where it takes into account the increase or decrease of Y value based on the change in X and add the bias term. The concept of balance between the weights and bias is well explained by James et al, (2023). Highly recommended to read through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c87de1-4419-47c9-a910-54fc3df6bc10",
   "metadata": {},
   "source": [
    "## Logistic Model Prediction (70/20/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94e9ade5-2344-408c-b110-ad7a8cfce170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "604f3522-9e6d-4fb3-8915-4b0b71d483e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Accuracy: 0.6667\n",
      "Sklearn Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"PISA2018_data.csv\")\n",
    "\n",
    "# Use one predictor variable (e.g., 'WEALTH')\n",
    "X = df['WEALTH'].values.reshape(-1, 1) \n",
    "y = df['LANGUAGE'].values  # Binary outcome variable ONLY!!\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(lr=0.001, n_iters=5000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Manual accuracy\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "manual_acc = accuracy(y_pred, y_test)\n",
    "print(f\"Manual Accuracy: {manual_acc:.4f}\")\n",
    "\n",
    "# Using sklearn metrics\n",
    "print(f\"Sklearn Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc46d7-57be-4078-ba6d-684937612c90",
   "metadata": {},
   "source": [
    "## Prediction Model Interpretation and Note\n",
    "#### Use one predictor variable (e.g., 'WEALTH')\n",
    "    df = pd.read_csv(\"PISA2018_data.csv\")\n",
    "    X = df['WEALTH'].values.reshape(-1, 1) \n",
    "    y = df['LANGUAGE'].values  # Binary outcome variable ONLY!!\n",
    "\n",
    "The dataset PISA2018_data.csv was loaded into a Pandas DataFrame, with 'WEALTH' as the predictor feature and 'LANGUAGE' as the binary outcome. 'WEALTH' was reshaped into a 2D (rows and columns) array as required by sci-kit-learn for input variables, which works just like transposing in the MS Excel. This solution was facilitated by the ChatGPT-4o model. I modified the original code by AssemblyAI (2022) to utilize scikit-learn metrics for accuracy comparison against a manual calculation, ensuring the correct equation.\n",
    "\n",
    "#### Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Just like in the linear regression model, the data was split into training and test sets using an 80-20 ratio. X_train and y_train are used to train the model, while X_test and y_test are reserved for evaluating the model’s performance. A fixed random_state = 42 works as though setting a seed in R, ensuring reproducibility in randomness.\n",
    "\n",
    "#### Train Logistic Regression Model (Scikit-learn, 2022)\n",
    "    clf = LogisticRegression(lr=0.001, n_iters=5000)\n",
    "    clf.fit(X_tran, y_train)\n",
    "\n",
    "A custom logistic regression model (`LogisticRegression`) was initialized with a learning rate of 0.001 and set to run for 5000 iterations. Trained on the dataset (X_train and y_train), the model learns the relationship between WEALTH and the binary outcome LANGUAGE. In contrast to a previous Random Forest model that failed due to a lack of my machine specifications, this smaller dataset indicates that variable adjustments do not significantly affect accuracy or processing speed. For larger datasets, modifying these parameters can optimize accuracy and processing speed according to your research needs.\n",
    "\n",
    "#### Make Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "The trained logistic regression model predicts the outcomes for the test data (X_test). These predictions (y_pred) are later used to calculate accuracy.\n",
    "\n",
    "#### Manual Accuracy\n",
    "    def accuracy(y_pred, y_test):\n",
    "        return np.sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "    manual_acc = accuracy(y_pred, y_test)\n",
    "    print(f\"Manual Accuracy: {manual_acc:.4f}\")\n",
    "\n",
    "A manual accuracy function is defined to compare predictions (y_pred) with actual test outcomes (y_test). I differentiated the outcome between the manual and the pre-existing function for an easy comparison. The accuracy is calculated as the proportion of correctly predicted values over the total number of observations (AssemblyAI, 2022).\n",
    "\n",
    "#### Using Sklearn Metrics\n",
    "    print(f\"Scikit-learn Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "The accuracy_score function from scikit-learn metrics is used to validate the manual accuracy calculation. Both results are printed and compared to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9578e-3dcf-404f-80aa-e13b664d5c8a",
   "metadata": {},
   "source": [
    "### References:\n",
    "AssemblyAI (Director). (2022). (2) How to implement Logistic Regression from scratch with Python—YouTube [YouTube]. https://www.youtube.com/watch?v=YYEJ_GUguHw\n",
    "\n",
    "Scilit-learn. (2024). 1.1. Linear Models. Scikit-Learn (User Guide). https://scikit-learn/stable/modules/linear_model.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52702b-7773-4004-9234-0d849eedf961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41f912-879b-4f79-9785-efa9cfc01d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
